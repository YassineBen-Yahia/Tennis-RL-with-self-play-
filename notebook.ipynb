{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install \"gymnasium[atari]\"\n!pip install \"pettingzoo[atari]\"\n!pip install pettingzoo stable-baselines3 supersuit\n!pip install autorom[accept-rom-license]\n!AutoROM --accept-license","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:49:56.025404Z","iopub.execute_input":"2025-09-10T14:49:56.026142Z","iopub.status.idle":"2025-09-10T14:50:09.423009Z","shell.execute_reply.started":"2025-09-10T14:49:56.026113Z","shell.execute_reply":"2025-09-10T14:50:09.422214Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (0.29.0)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.14.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\nRequirement already satisfied: shimmy<1.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->gymnasium[atari]) (2.4.1)\nRequirement already satisfied: ale-py~=0.8.1 in /usr/local/lib/python3.11/dist-packages (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (0.8.1)\nRequirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[atari]) (6.5.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->gymnasium[atari]) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->gymnasium[atari]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->gymnasium[atari]) (2024.2.0)\nRequirement already satisfied: pettingzoo[atari] in /usr/local/lib/python3.11/dist-packages (1.24.0)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (1.26.4)\nRequirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (0.29.0)\nRequirement already satisfied: multi-agent-ale-py==0.1.11 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (0.1.11)\nRequirement already satisfied: pygame==2.3.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo[atari]) (2.3.0)\nRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (3.1.1)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (4.14.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo[atari]) (0.0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo[atari]) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo[atari]) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo[atari]) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo[atari]) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo[atari]) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo[atari]) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->pettingzoo[atari]) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->pettingzoo[atari]) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->pettingzoo[atari]) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->pettingzoo[atari]) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->pettingzoo[atari]) (2024.2.0)\nRequirement already satisfied: pettingzoo in /usr/local/lib/python3.11/dist-packages (1.24.0)\nRequirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.1.0)\nRequirement already satisfied: supersuit in /usr/local/lib/python3.11/dist-packages (3.9.3)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo) (1.26.4)\nRequirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from pettingzoo) (0.29.0)\nRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.7.2)\nRequirement already satisfied: tinyscaler>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from supersuit) (1.2.8)\nRequirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo) (4.14.0)\nRequirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium>=0.28.0->pettingzoo) (0.0.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.21.0->pettingzoo) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (2025.5.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13->stable-baselines3) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.2)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.58.4)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.2.1)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.0.9)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->pettingzoo) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.21.0->pettingzoo) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.21.0->pettingzoo) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.21.0->pettingzoo) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.21.0->pettingzoo) (2024.2.0)\nRequirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.6.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.4)\nRequirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (0.6.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2025.6.15)\nAutoROM will download the Atari 2600 ROMs.\nThey will be installed to:\n\t/usr/local/lib/python3.11/dist-packages/AutoROM/roms\n\t/usr/local/lib/python3.11/dist-packages/multi_agent_ale_py/roms\n\nExisting ROMs will be overwritten.\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"import gymnasium as gym\nfrom gymnasium import spaces\nimport ale_py \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:50:09.424658Z","iopub.execute_input":"2025-09-10T14:50:09.424864Z","iopub.status.idle":"2025-09-10T14:50:09.428778Z","shell.execute_reply.started":"2025-09-10T14:50:09.424845Z","shell.execute_reply":"2025-09-10T14:50:09.428173Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"import os\n\nimport numpy as np\nimport supersuit\nfrom stable_baselines3 import PPO\nfrom stable_baselines3.common.vec_env import DummyVecEnv\nfrom pettingzoo.atari import tennis_v3\nfrom collections import deque\nfrom supersuit import pettingzoo_env_to_vec_env_v1, concat_vec_envs_v1\nfrom pettingzoo.utils.conversions import aec_to_parallel\nfrom pettingzoo.utils import AECEnv\nfrom pettingzoo.utils.wrappers import BaseParallelWrapper\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T14:50:09.432851Z","iopub.execute_input":"2025-09-10T14:50:09.433051Z","iopub.status.idle":"2025-09-10T14:50:09.446748Z","shell.execute_reply.started":"2025-09-10T14:50:09.433037Z","shell.execute_reply":"2025-09-10T14:50:09.446273Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"class SelfPlayWrapper(gym.Env):\n    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 30}\n    \n    def __init__(self, env, opponent_policy):\n        super().__init__()\n        self.env = env\n        self.opponent_policy = opponent_policy\n\n        self.learning_agent = self.env.possible_agents[0]\n        self.opponent_agent = self.env.possible_agents[1]\n\n        # Convert to gymnasium space (float32, normalized)\n        orig_space = self.env.observation_space(self.learning_agent)\n        self.observation_space = spaces.Box(\n            low=0.0, high=1.0, shape=orig_space.shape, dtype=np.float32\n        )\n\n        self.action_space = self.env.action_space(self.learning_agent)\n\n    def reset(self, **kwargs):\n        result = self.env.reset(**kwargs)\n\n        # unpack if tuple\n        if isinstance(result, tuple):\n            obs_dict = result[0]  # first item is the observations dict\n            info_dict = result[1] if len(result) > 1 else {}\n        else:\n            obs_dict = result\n            info_dict = {}\n    \n        self.last_obs = obs_dict  # for opponent\n    \n        obs = obs_dict[self.learning_agent].astype(np.float32) / 255.0\n        return obs, info_dict  # <-- SB3 expects exactly 2 values here\n\n\n    def step(self, action):\n        # Build actions dict\n        actions = {self.learning_agent: action}\n    \n        # Opponent acts\n        if self.opponent_policy is None:\n            opp_action = self.env.action_space(self.opponent_agent).sample()\n        else:\n            opp_obs = self.last_obs[self.opponent_agent]\n            opp_action, _ = self.opponent_policy.predict(opp_obs, deterministic=True)\n        actions[self.opponent_agent] = opp_action\n    \n        # Step the parallel env\n        obs_dict, rewards, dones, infos = self.env.step(actions)[:4]  # returns (obs_dict, infos_dict)\n        \n        # Update last_obs for opponent\n        self.last_obs = obs_dict\n    \n        # Extract learning agent info\n        obs = obs_dict[self.learning_agent].astype(np.float32) / 255.0\n        info = infos.get(self.learning_agent, {})\n        if not isinstance(info, dict):\n            info = {}\n        reward = rewards.get(self.learning_agent, 0.0)  \n        terminated = dones.get(self.learning_agent, False)\n        truncated = False  # optional\n        \n    \n        return obs, reward, terminated, truncated, info\n\n        \n    def set_opponent(self, opponent_policy):\n        self.opponent_policy = opponent_policy\n\n\n\ndef make_env(opponent_policy, render_mode=None):\n    def _init():\n        base_env = tennis_v3.parallel_env(render_mode=render_mode or \"rgb_array\")\n        env = SelfPlayWrapper(base_env, opponent_policy)\n        return env\n    return _init","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:52:06.948866Z","iopub.execute_input":"2025-09-10T15:52:06.949443Z","iopub.status.idle":"2025-09-10T15:52:06.958927Z","shell.execute_reply.started":"2025-09-10T15:52:06.949420Z","shell.execute_reply":"2025-09-10T15:52:06.958366Z"}},"outputs":[],"execution_count":90},{"cell_type":"code","source":"class SelfPlayPPO:\n    def __init__(self, policy_pool_size=5, switch_frequency=1000):\n        self.policy_pool = deque(maxlen=policy_pool_size)\n        self.switch_frequency = switch_frequency\n        self.training_steps = 0\n        # Start with random opponent (can be PPO, random policy, etc.)\n        dummy_opponent = PPO(\"MlpPolicy\", gym.make(\"ALE/Tennis-v5\"))\n        self.env = DummyVecEnv([make_env(dummy_opponent)])  # Manually vectorize\n        self.model = PPO(\"MlpPolicy\", self.env, verbose=1, tensorboard_log=\"./tb_logs\")\n        self.env.envs[0].set_opponent(self.model)\n    \n    def train(self, total_timesteps=10000):\n        while self.training_steps < total_timesteps:\n            self.model.learn(total_timesteps=self.switch_frequency, reset_num_timesteps=False)\n            self.training_steps += self.switch_frequency\n            \n            # Save snapshot of current policy into pool\n            if self.training_steps % self.switch_frequency == 0:\n                policy_path = f\"policy_pool/step_{self.training_steps}.zip\"\n                os.makedirs(\"policy_pool\", exist_ok=True)\n                self.model.save(policy_path)\n                self.policy_pool.append(policy_path)\n                print(f\"Added policy to pool: {policy_path}\")\n                \n                if len(self.policy_pool) > 0 and np.random.random() < 0.7:\n                    opponent_path = np.random.choice(list(self.policy_pool))\n                    new_opponent = PPO.load(opponent_path, env=self.env)\n                else: \n                    new_opponent = self.model\n                \n                # Swap opponent\n                self.env.envs[0].set_opponent(new_opponent)\n                print(f\"Opponent ID: {id(self.env.envs[0].opponent_policy)}\")\n            \n    def evaluate(self, n_episodes=10, render=False):\n        rewards = []\n    \n        for ep in range(n_episodes):\n            print(n_)\n            obs_dict = self.env.reset()  # returns dict\n            ep_reward = 0\n            done = False\n    \n            while not done:\n                # normalize obs to float32 [0,1]\n                obs = obs_dict[self.env.learning_agent].astype(np.float32) / 255.0\n    \n                action, _ = self.model.predict(obs, deterministic=True)\n    \n                obs_dict, rewards_dict, dones_dict, infos_dict = self.env.step(action)\n    \n                ep_reward += rewards_dict.get(self.env.learning_agent, 0.0)\n                done = dones_dict.get(self.env.learning_agent, False)\n    \n                if render:\n                    self.env.render()  # or env.envs[0].render()\n    \n            rewards.append(ep_reward)\n    \n        mean_reward = np.mean(rewards)\n        print(f\" -Evaluation over {n_episodes} episodes: mean_reward = {mean_reward:.2f}\")\n        return mean_reward\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:52:12.190553Z","iopub.execute_input":"2025-09-10T15:52:12.191207Z","iopub.status.idle":"2025-09-10T15:52:12.200071Z","shell.execute_reply.started":"2025-09-10T15:52:12.191184Z","shell.execute_reply":"2025-09-10T15:52:12.199325Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"trainer = SelfPlayPPO()\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:11:40.619416Z","iopub.execute_input":"2025-09-10T15:11:40.619906Z","iopub.status.idle":"2025-09-10T15:13:43.569000Z","shell.execute_reply.started":"2025-09-10T15:11:40.619881Z","shell.execute_reply":"2025-09-10T15:13:43.568043Z"}},"outputs":[{"name":"stdout","text":"Using cuda device\nLogging to ./tb_logs/PPO_0\n-----------------------------\n| time/              |      |\n|    fps             | 316  |\n|    iterations      | 1    |\n|    time_elapsed    | 6    |\n|    total_timesteps | 2048 |\n-----------------------------\nAdded policy to pool: policy_pool/step_1000.zip\nOpponent ID: 134186828162768\nLogging to ./tb_logs/PPO_0\n-----------------------------\n| time/              |      |\n|    fps             | 315  |\n|    iterations      | 1    |\n|    time_elapsed    | 6    |\n|    total_timesteps | 4096 |\n-----------------------------\nAdded policy to pool: policy_pool/step_2000.zip\nOpponent ID: 134186828359376\nLogging to ./tb_logs/PPO_0\n-----------------------------\n| time/              |      |\n|    fps             | 317  |\n|    iterations      | 1    |\n|    time_elapsed    | 6    |\n|    total_timesteps | 6144 |\n-----------------------------\nAdded policy to pool: policy_pool/step_3000.zip\nOpponent ID: 134186780050064\nLogging to ./tb_logs/PPO_0\n-----------------------------\n| time/              |      |\n|    fps             | 315  |\n|    iterations      | 1    |\n|    time_elapsed    | 6    |\n|    total_timesteps | 8192 |\n-----------------------------\nAdded policy to pool: policy_pool/step_4000.zip\nOpponent ID: 134187900105168\nLogging to ./tb_logs/PPO_0\n------------------------------\n| time/              |       |\n|    fps             | 315   |\n|    iterations      | 1     |\n|    time_elapsed    | 6     |\n|    total_timesteps | 10240 |\n------------------------------\nAdded policy to pool: policy_pool/step_5000.zip\nOpponent ID: 134186757514640\nLogging to ./tb_logs/PPO_0\n------------------------------\n| time/              |       |\n|    fps             | 313   |\n|    iterations      | 1     |\n|    time_elapsed    | 6     |\n|    total_timesteps | 12288 |\n------------------------------\nAdded policy to pool: policy_pool/step_6000.zip\nOpponent ID: 134194335154512\nLogging to ./tb_logs/PPO_0\n------------------------------\n| time/              |       |\n|    fps             | 315   |\n|    iterations      | 1     |\n|    time_elapsed    | 6     |\n|    total_timesteps | 14336 |\n------------------------------\nAdded policy to pool: policy_pool/step_7000.zip\nOpponent ID: 134187947252496\nLogging to ./tb_logs/PPO_0\n------------------------------\n| time/              |       |\n|    fps             | 317   |\n|    iterations      | 1     |\n|    time_elapsed    | 6     |\n|    total_timesteps | 16384 |\n------------------------------\nAdded policy to pool: policy_pool/step_8000.zip\nOpponent ID: 134186772326352\nLogging to ./tb_logs/PPO_0\n------------------------------\n| time/              |       |\n|    fps             | 317   |\n|    iterations      | 1     |\n|    time_elapsed    | 6     |\n|    total_timesteps | 18432 |\n------------------------------\nAdded policy to pool: policy_pool/step_9000.zip\nOpponent ID: 134186754576720\nLogging to ./tb_logs/PPO_0\n------------------------------\n| time/              |       |\n|    fps             | 314   |\n|    iterations      | 1     |\n|    time_elapsed    | 6     |\n|    total_timesteps | 20480 |\n------------------------------\nAdded policy to pool: policy_pool/step_10000.zip\nOpponent ID: 134186828359376\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"trainer.evaluate(n_episodes=10, render=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:52:20.239677Z","iopub.execute_input":"2025-09-10T15:52:20.239930Z","iopub.status.idle":"2025-09-10T15:52:25.075340Z","shell.execute_reply.started":"2025-09-10T15:52:20.239912Z","shell.execute_reply":"2025-09-10T15:52:25.074393Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1814477855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/2233791926.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, n_episodes, render)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mep_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mep_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Extract from array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    553\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \"\"\"\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m         \u001b[0;31m# Convert to numpy, and reshape to the original action shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36m_predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTaken\u001b[0m \u001b[0maction\u001b[0m \u001b[0maccording\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         \"\"\"\n\u001b[0;32m--> 679\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_distribution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mget_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    711\u001b[0m         \"\"\"\n\u001b[1;32m    712\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi_features_extractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         \u001b[0mlatent_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward_actor\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_critic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":92},{"cell_type":"code","source":"model= PPO.load('policy_pool/step_9000.zip')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:24:11.729948Z","iopub.execute_input":"2025-09-10T15:24:11.730186Z","iopub.status.idle":"2025-09-10T15:24:12.516375Z","shell.execute_reply.started":"2025-09-10T15:24:11.730170Z","shell.execute_reply":"2025-09-10T15:24:12.515784Z"}},"outputs":[],"execution_count":71},{"cell_type":"code","source":"from stable_baselines3.common.evaluation import evaluate_policy\n\ndummy_opponent = PPO(\"MlpPolicy\", gym.make(\"ALE/Tennis-v5\"))\nenv = DummyVecEnv([make_env(dummy_opponent)])\nenv.envs[0].set_opponent(model)\n\nmean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\nprint(f\"Mean reward: {mean_reward}, Std: {std_reward}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-10T15:51:15.731760Z","iopub.execute_input":"2025-09-10T15:51:15.732014Z","iopub.status.idle":"2025-09-10T15:51:16.657232Z","shell.execute_reply.started":"2025-09-10T15:51:15.731993Z","shell.execute_reply":"2025-09-10T15:51:16.656227Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3450751055.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_opponent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmean_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Mean reward: {mean_reward}, Std: {std_reward}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mcurrent_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mcurrent_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_envs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mobservations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mepisode_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvObs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_seeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Seeds are only used once\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/318577866.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mobs_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_obs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_agent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m  \u001b[0;31m# normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"],"ename":"TypeError","evalue":"tuple indices must be integers or slices, not str","output_type":"error"}],"execution_count":89}]}